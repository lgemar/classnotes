10/22:

??Computer Architecture??
==> Def'n: architecture = interface between hardware and software in a 
	computer system
	: System design -> focus on Busses, I/O, Memory Controllers, etc
	: Instruction Set Architecture
	: Implementation -> specific hardware design 
==> Instruction Set Architecture (MIPS for this class)
	--> "Contract" between the programmer and the compiler
	--> operations that the processor can execute
	--> data transfer mechanisms
	--> control mechanisms
==> Design Choices
	--> Which operations supported? 
	--> Types of operands? 
	--> Types of operand storage? 
	--> Implicit vs. explicit operands?
==> Reduced Instruction Set Computing vs Complex Instruction Set Computing
	: ARM -> example of RISC, complexity in the compiler
		--> reg to reg operation (load / store)
		--> basic functions
		--> fixed instruction length
	: CISC -> example is x86, complexity is in the hardware
		--> Reg to mem operation
		--> complex functions (e.g. string comparison)
		--> variable length instructions
==> Addressing Modes
	: mapping must be done from virtual to real address
	: registers corresponding to variables in C program ($s0, $s1, ...)
	: temporary registers need to compile into MIPS ($t0, $t1 ...)
	: add/sub $s2, $s0, $s1 # $s2 = $s0 +/- $s1
	: lw == (load word)
	: sw == (store word)
	: lw $s1, k($s2) # $s1 = A[k] where $s2 is base of array
	: sw $s1, k($s2) # A[k] = $s1 where $s2 is base of array
10/20: 
??Computer Architecture??
==> Interface between the software and the low level logic hardware
==> Architecture = interface between hardware and software in a computer
	system
==> Implementation = specific hardware design
==> In GPU's the compiler is "embedded in the hardware", meaning instruction
	set is actively accessed at run time by the GPU companies software
==> Simple computer design
	: CPU -> cache -> memory bus -> memory -> i/o bus -> io devices
==> Key principles
	==> Multiple interpretations for a bit string
		: two's complement
		: single precision floating point
		: instruction
	==> Programs manipulated in memory, just like data
		: high-level -> compilation -> assembly -> register allocation -> 
		  machine language -> assembly -> binary instruction -> execution
==> Decode instructions into multiple categories
	1.) Arithmetic/Logic instructions
		: read 2 register values, perform operation, store result
	2.) Memory Instructions
		: load and store data into memory
	3.) Control Flow instructions
		: change of flow based on comparison

??How does CPU interface to Memory and I/O systems??
==> Instruction execution follows specific sequence
	: instruction fetch -> operand fetch -> execution -> write result
==> Possible external operations
	: memory read/ write
	: I/O device read or write
	; interrupt acknowledge

??How do Buses work??
==> Bus = one set of wires used as a shared communication link between 
	multiple subsystems of computer
	: pros ->
		-> versatility
		-> may limit i/o capacity
	: speed is limited by length of bus and number of devices connected
	: bus speed limited by range of devices
	: General structure of a bus
		-> set of data lines
		-> set of control lines
	: Bus transaction
		-> read
		-> write
		-> Nowadays, buses are beginning to send / recieve packets
	: Timing Schemes
		-> synchronous: clock is included in the control lines
			-> advantages
				: straightforward implementation
				: bus runs fast
			-> disadvantages
				: every device must run at the same speed
		-> asynchronous: no clock, use handshake protocol (request-reply-ack)
	: Asynchronous
		t0) master asserts lines
		t1) master waits and asserts req
		t2) device asserts Ack
		t3) master releases Req
		t4) device releases ack

10/8:
??LEARN THREE THINGS??
==> Registers and Wires
==> Always blocks
==> Finite State Machine

??REGISTERS AND WIRES??
==> Wires
	==> No state
	==> pieces of metal
	==> Results of digital logic and combinational 
	==> Use to compute values
==> Registers
	==> Hold state
	==> Use to save data at the end of every cycle
	==> Think of D flip flop from homework
==> Multiply and Add Circuit: Y = A * B + C;
	==> wire A, B, C, Y, temp;
		assign temp = A * B;
		assign Y = temp + c;
	==> // Example with registers
		wire A, B, C, Y;
		wire temp_n;
		reg temp;
		always@(posedge clk) begin
			temp <= temp_n;
		end
		assign temp_n = A * B;
		assign Y = temp + c;
==> ALWAYS BLOCKS
	==> Always @ (sensitivity list)
		==> every time any of the events in the list happens, the logic fires
	==> Two types of lists
		==> Combinational
			==> assign statements at top of file
		==> Sequential
			==> Triggered by a fixed event
	==> Blocking assignment: "="
		==> wire assignment should be used using blocking statements
		==> Statements using "=" execute procedurally
	==> Non-blocking assignment: "<="
		==> Use these in the always sections to save values to registers in
			parallel
==> Design Tips
	==> Name wires <var>_next and regs <var>
	==> Group your always blocks at the top of the file
	==> Have all combinatinoal logic at the bottom of the file
	==> Clean coding reinfornces paradigm

==> Finite State Machines
10/6:
??FINITE STATE MACHINES??
==> Steps:
	1.) Understand
	2.) Obtain abstract representation of FSM
		==> draw state transition diagrams
		==> or alternative state machine representation
	3.) Perform state minimization
	4.) Perform state assignment
		==> encode states
		==> build state transition table
	5.) Implement

==> STATE MINIMIZATION
	==> Two states are equivalent if: 
		==> output behavior is identical for all inputs
		==> next state same or equivalent for all inputs
	==> You could build full state transition diagram and look 
		for cases where you could merge states

==> STATE ASSIGNMENT
	==> How to represent state names in binary encoding
	==> Hueristics: 
		==> minimum bit change
		==> one hot encoding: num(flip flops) equal to # of states
			==> Example for three states: 
				001, 010, 100 -> only one flip-flop is active for each state
			==> Binary encoding could be: 
				00, 01, 10 -> log2(N) flip flops used
		==> Minimize state variables that change on each transition
		==> If unused state bits, utilize these bits to achieve the above
			goals
	==> FSM Partitioning
		==> KEY IDEA: introduction of idle states
			==> Generate two smaller finite state machines that have 
				links to transfer from one machine to the other

==> EXAMPLE: Simple vending machine
	==> Step 1: Understand the problem
		==> Vending machine dispenses gum given 15c or more
		==> Inputs: nickel, dime, reset, clk
		==> Outputs: dispense
		==> What inputs lead to an output? 
			==> 3N; D,N; N,D; D,D; N,N,D
	==> Step 2: Abstract Representation
	==> Step 4: State Assignment
	==> Now onto the Gate Implementation
		==> The present state is the output of the flip flop, the next
			state is the input to the flip flop
			==> Calculating the next state will involve some gate logic
				between the inputs and the current state outputs

==> Timing
	==> Flip flop samples input (D) at clock edge
	==> D must be stable when it is sampled
	==> Similar to photograph, D must be stable around the clock edge
	==> If D is changing when it is sampled, errors can occur
	==> Output timing constraints
		==> Output is undefined for a certain amount of time after the 
			clock has fired
	==> Dynamic Discipline
		==> The input to a synchronous sequential circuit must be stable
			during the aperature time
		==> Tpcq and Tsetup are determined by a flip-flop designer
		==> However, you can change your combinational logic to make it
			faster or slower
	==> What is the Hold Time Constraint
		==> Minimum delay through the circuit
		==> Why do I care? 
			==> The inputs must stay the same during the hold time
